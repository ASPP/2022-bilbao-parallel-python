{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serial Python\n",
    "\n",
    "Before we get into parallel processing, let's first consider how the type of problems we'll consider are typically solved in a serial context. We'll focus on problems which always have the following form: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_science(x):\n",
    "    \"\"\"For example:\n",
    "    - training a neural network (hyperparameter tuning!)\n",
    "    - getting results from a database\n",
    "    - scraping some websites\n",
    "    - reading files\n",
    "    - sampling monte-carlo style\n",
    "    \"\"\"\n",
    "    return x ** 2  # we don't really do anything ;)\n",
    "\n",
    "results = []\n",
    "input_data = range(10)\n",
    "for x in input_data:\n",
    "    results.append(do_science(x))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This typical structure (or \"smell\") is pretty common and most likely all of you have something similar somewhere in your code. It's an excellent opportunity for leveraging parallelism to speed things up. However, first we'll rewrite this code using the builtin `map` function, it makes the code more compact and it will be easier to make this run in parallel. `map` takes a function and an iterable and applies the function to each item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when applying a function to a bunch of data, maybe you would use list comprehension\n",
    "results = [do_science(x) for x in input_data]\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# however, here we use map to later explore its parallel implementations\n",
    "results = map(do_science, input_data)\n",
    "results = list(results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(watch out: `map` returns a generator (->advanced python), you need to convert it to a list explicitly)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "- What's the return type of `map`? -> \"generator\" -> needs to be exhausted with the `list` constructor\n",
    "- Can we also compute the *sum* of numbers `0..9` using map? -> no(t trivially). results of operations depend on each other. this is a contra-indicator for \"embarrassingly parallel\" problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embarassingly parallel Python\n",
    "This type of problem is referred to as \"embarrassingly parallel\" problems. This indicates that they can be easily parallelized across threads or processes as they do not require interaction while running (they can also be run in serial!). For these types of problems, we can use the builtin `multiprocessing` module. It supports parallel versions of `map` which can be run either in parallel threads or parallel processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel threads\n",
    "We first work with the `ThreadPool` available from the `multiprocessing.pool` module. We assume CPython in which the GIL prevent several threads from executing in parallel. However, for some use cases, in particular those which are **I/O bound**, threading can be very useful. Consider for example obtaining data from some database: you would like to query a couple of measurements, and completing each of these queries may take some processing time on the server. Here we mimick this server-side processing time by merely sleeping (which releases the GIL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.pool import ThreadPool\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_database(x):\n",
    "    \"\"\"Query your database to retrieve awesome measurements.\"\"\"\n",
    "    print(f'querying data {x} start')\n",
    "    time.sleep(x)  # mimicks (input-dependent) server-side processing\n",
    "    y = x ** 2\n",
    "    print(f'querying data {x} end')\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [1, 8, 1.5, 2]  # some dummy queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we use the builtin `map` function to perform the database query for each item in l:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = list(map(query_database, input_data))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations?\n",
    "- queries processed in serial, one after the other\n",
    "- total duration is the sum of the duration of each query "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use use `ThreadPool` do perform these queries using multiple threads (here the `processes` argument actually refers to the number of threads).\n",
    "\n",
    "First, let's figure out how many CPUs we have available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# pool = ThreadPool(processes=2)\n",
    "# result = pool.map(query_database, input_data)\n",
    "# pool.close()\n",
    "\n",
    "with ThreadPool(processes=2) as pool:  # context manager providing a `ThreadPool` instance\n",
    "    result = pool.map(query_database, input_data)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations?\n",
    "- results are identical to serial processing of queries; good!\n",
    "- total duration is reduced: work (here: waiting for results) is distributed across threads -> how is this possible? GIL! -> sleep doesn't hold the GIL, mimicking waiting for, e.g., network I/O\n",
    "- allocation: queries are performed in order; thread 0 works on query 0, thread 1 on query 1, thread 0 on the rest while thread 1 is busy with query 1\n",
    "- caveat: optimal number of threads may be difficult to determine (depends on use case! more threads also means more switching!)\n",
    "- caveat: load is not automatically ordered optimally (`ThreadPool` can not know how long each query takes); in our example if long query is the last, total duration increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [1, 1.5, 2, 8]  # some dummy queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with ThreadPool(processes=2) as pool:\n",
    "    result = pool.map(query_database, input_data)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thread-parallel(?) number crunching\n",
    "Now let's consider a compute-intense number-crunching task, for example tuning hyperparameters our fancy neural network model to squeeze out the additional 0.0002% increase in accuracy. Here we mimick the training by merely counting down from a large number (let's avoid cognitive overhead)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(x):\n",
    "    \"\"\"'Train' your favourite neural network model.\"\"\"\n",
    "    print(f'training with {x} start')\n",
    "    n = x * 2e7  # mimick compute-intense training\n",
    "    while n > 0:\n",
    "        n -= 1\n",
    "    y = x ** 2\n",
    "    print(f'training with {x} end')\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [1, 8, 1.5, 2]  # some dummy simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, first, we use the builtin `map` function to perform the number crunching serially for each item in l:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = list(map(train_neural_network, input_data))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- number crunching causes high CPU load (surprise! ;) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use again use `ThreadPool` do parallelize this work using two threads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with ThreadPool(processes=2) as pool:\n",
    "    result = pool.map(train_neural_network, input_data)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations?\n",
    "- runtime (almost) identical to serial execution: GIL prevents simultaneous number crunching. :'(\n",
    "\n",
    "Questions:\n",
    "- what is the return type of `pool.map`? -> list\n",
    "- why is this different than the return type of `map`? -> normal map: delayed execution is desired, with pool you want to do the computation immediately (and in parallel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel processes\n",
    "For such tasks, we use the `ProcessPool`. In contrast to the `ThreadPool` this distributes work across multiple processes running separate instances of the Python interpreter. This allows you to circumvent the limitations of the GIL and achieve truly parallel code execution. For use cases which are **compute bound**, it is an excellent, simple-to-use option. As already introduced above, these use cases may include numerical simulations, sampling methods etc. Unfortunately, using multiple processes introduces some downsides, such as some overhead (time & memory) for launching processes and increased memory consumption (e.g., duplication of data; warning: depends on implementation and use case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.pool import Pool as ProcessPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with ProcessPool(processes=2) as pool:  # context manager providing a `Pool` instance\n",
    "    result = pool.map(train_neural_network, input_data)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations?\n",
    "- results are identical to serial and threaded execution; good!\n",
    "- runtime is reduced compared to both serial and threaded execution\n",
    "- increased CPU load on multiple cores\n",
    "- caveat: as before, no automatic load balancing, tasks are executed in order\n",
    "\n",
    "Questions:\n",
    "- What is the fundamental difference between threads and processes? -> memory shared across threads, by default not across processes\n",
    "- How is data communicated between threads? -> shared memory (direct access)\n",
    "- How is data communicated between processes? -> sending serialized data from one process to the other (in Python: `pickle` -> problems with stuff that's not picklable), or set up shared memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel speedup\n",
    "\n",
    "So how much faster does my code become when I'm increasing the number of processes? Here we investigate the relative speedup ($T_\\textrm{parallel} / T_\\textrm{serial}$) for an increasing number of processes. We use the same compute-bound function as before, but remove some of the annoying output and make it a bit shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def train_neural_network(x):\n",
    "    \"\"\"Train your favourite neural network model.\"\"\"\n",
    "    n = x * 2e6  # mimick compute-intense training\n",
    "    while n > 0:\n",
    "        n -= 1\n",
    "    y = x ** 2\n",
    "    return y\n",
    "\n",
    "\n",
    "input_data = [2] * 16  # some dummy simulations of equal duration\n",
    "times = []\n",
    "n_processes = np.arange(1, multiprocessing.cpu_count() + 4)\n",
    "for n in n_processes:\n",
    "    t0 = time.time()\n",
    "    with ProcessPool(processes=n) as pool:\n",
    "        result = pool.map(train_neural_network, input_data)\n",
    "    times.append(time.time() - t0)\n",
    "\n",
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "times = np.array(times)\n",
    "fig, axes = plt.subplots()\n",
    "axes.plot(n_processes, 1.0 * n_processes, color='k', linestyle='--', label='ideal')\n",
    "axes.plot(n_processes, times[0] / times, marker='o', label='measured')\n",
    "axes.set_xlabel(r'$n$ processes')\n",
    "axes.set_ylabel('relative speedup')\n",
    "fig.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations\n",
    "- perfect speedup up to X processes, (on some machines) good speedup until ~Y processes with decreasing benefits\n",
    "- no (significant) benefits for more processes\n",
    "- rule of thumb: benefits up to number of cores (OS also needs some compute: context switching; also hyperthreading does not seem to work well in my experience)\n",
    "\n",
    "Questions:\n",
    "- Can we combine `ProcessPools` with `ThreadPools` in the worker processes? -> yes, but benefit depends on use case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiprocessing, but with numpy\n",
    "Let's consider a similar example as before (number crunching!) but now we leverage additional libraries to speed up the computation. In this example we will use numpy to perform maxtrix multiplications (dot products). To be fast, numpy (and many other libraries) rely on implementations of computationally expensive function in *compiled* language (e.g., C++). This is great, but unfortunately may interfere with multiprocessing at the Python level, since the compiled function can leverage parallelization too! To illustrate this point, let's consider a compute-intense function which just performs matrix multiplications. Again, we will compare the runtime of the serial with the parallel version (using multiple processes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "from multiprocessing.pool import Pool as ProcessPool, ThreadPool\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "def train_neural_network(m):\n",
    "    \"\"\"Train your favourite neural network model.\"\"\"\n",
    "    # want to use multiprocessing for much serial code\n",
    "    # would not want to use multiprocessing if most is parallel code\n",
    "    y = np.mean(np.dot(m, m))  # mimick compute-intense training\n",
    "    return y\n",
    "\n",
    "n = 5_000\n",
    "# generate some random matrices as input\n",
    "input_data = [np.random.randn(n, n), np.random.randn(n, n), np.random.randn(n, n), np.random.randn(n, n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.027806310017984486,\n",
       " -0.0035359750666366786,\n",
       " -0.022726991942128072,\n",
       " -0.002367779511304798]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## %%time\n",
    "result = list(map(train_neural_network, input_data))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with ProcessPool(processes=4) as pool:\n",
    "    result = pool.map(train_neural_network, input_data)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "- Duration does not change (might actually be a bit longer) if we use multiple processes.\n",
    "- High load on all CPUs *even for the serial computation*. -> numpy parallelizes at a lower level\n",
    "- Solution: limit the number of threads numpy can use via the `OMP_NUM_THREADS` environment variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "https://www.youtube.com/watch?v=AG1soUh4-nU\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
